---
title: "Team 5 Codebook - Baby Cry Project"
author: "Brandon Erickson, Bryan Mui, Carolynn Rui, Jeremy Reyes, Ivy Le, Zijia Zhang"
execute:
  cache: true
format:
  pdf:
    geometry: left=0.3in, right=0.3in, top=0.3in, bottom=0.3in
    keep-tex: true
    include-in-header:
       text: |
         \usepackage{fvextra}
         \DefineVerbatimEnvironment{Highlighting}{Verbatim}{breaklines,commandchars=\\\{\}}
         \DefineVerbatimEnvironment{OutputCode}{Verbatim}{breaklines,commandchars=\\\{\}}
---


# Libraries:

```{r}
library(tidyverse)
library(readxl)
library(dplyr)
library(ggplot2)
library(ggpubr)
library(janitor)
library(randomForest)
library(mice)
library(caret)
library(xgboost)
library(MLmetrics)
library(reshape2)
library(pROC)
library(factoextra)
library(e1071)
library(stats)
library(igraph)
library(kernlab)
library(dbscan)
library(knitr)
library(mclust)
library(cluster)
library(nnet)
library(GGally)
library(skimr)
```


```{r}
full_data_odd <- read_csv("./Data/filtered_full_data_odd.csv")
demographics <- read_csv("./Data/demographics_students.csv")
```

# Cleaning:

Find the groups of demographics dataset:

```{r}
demographics %>%
  group_by(Reason) %>%
  summarise(count = n())
```

I see some no email and no gender and no age. clean the demographics data

```{r}
# demographics <- demographics %>%
#   filter(ID != 'NO-EMAIL', Age != 'NO-AGE', Gender != 'NO-GENDER')
# 
# demographics %>%
#   group_by(Reason) %>%
#   summarise(count = n())
```

```{r}
full_data_odd %>%
  group_by(ParentFile) %>%
  summarise(count = n())
```

```{r}
demographics %>%
  group_by(Reason) %>%
  summarise(count = n())
```

Join the two datasets:
```{r}
# # Filter out email and no email separately 
# demographics_no_em <- demographics %>%
#   filter(ID == "NO-EMAIL")
# demographics_em <- demographics %>%
#   filter(ID != "NO-EMAIL")

# full_data_no_em <- full_data_odd %>%
#   filter(str_detect(Filename, "^NO-EMAIL"))
# full_data_em <- full_data_odd %>%
#   filter(!str_detect(Filename, "^NO-EMAIL"))

# For the email, join by ID
full_data_mod <- full_data_odd %>%
  #mutate(ID = str_split(Filename, "_")[1]) %>%
  separate(Filename, into = paste0("Comp", 1:8), sep = "_") %>%
  relocate(paste0("Comp", 1:8), .before = ParentFile) %>%
  rename(ID = `Comp1`, Reason = Comp2, Age = Comp3, Gender = Comp4, Date = Comp5, Sample = Comp6) %>%
  select(-Comp7, -Comp8)

# join_em <- left_join(full_data_em, demographics_em)

# For the no email, join by the sample ID plus date

```

```{r}
# write_csv(full_data_mod, "full_data_mod.csv")
```

# Question 1: EDA

```{r}
# reading in data
full_data_odd <- read.csv("./Data/filtered_full_data_odd.csv", header = TRUE)
demographics <- read.csv("./Data/demographics_students.csv", header = TRUE)
```

```{r}
# analyze rows
head(demographics)
```

## Reason Distribution Bar Plot
```{r}
# see how the distributions stack up
table(demographics$Reason)

barplot(table(demographics$Reason),
        col = "lightgreen",
        main = "Cry Type Distribution",
        ylab = "Count")

```

## Reason with Gender Proportion
```{r}
# see how the reasons vary with gender
table(demographics$Gender, demographics$Reason)

library(ggplot2)
ggplot(demographics, aes(x = Reason, fill = Gender)) +
  geom_bar(position = "fill") +
  labs(title = "Gender Proportion by Cry Type", y = "Proportion") +
  theme_minimal()
```

## Chi-Square
```{r}
# chi square test between gender and reason
chisq.test(table(demographics$Gender, demographics$Reason))
```

## ANOVA w/ removed missing ages
```{r}
# anova between age and reason
# convert missing ages to NA
demographics$Age[demographics$Age == "NO-AGE"] <- NA
demographics$Age <- as.numeric(demographics$Age)
anova_result <- aov(Age ~ Reason, data = demographics)
summary(anova_result)

# visuaally see age by reason
boxplot(Age ~ Reason, data = demographics,
        main = "Age Distribution by Cry Type",
        xlab = "Cry Type",
        ylab = "Age (Months)",
        col = "lightgreen")
```

## Scree Plot by Cry Acoustics Dimensions
```{r}
# see how the acoustic feastures measure
library(factoextra)
# get numeric only
acoustic_features <- full_data_odd %>%
  select(where(is.numeric)) %>%
  na.omit()  

#scale for scree plot
scaled_features <- scale(acoustic_features)
pca <- prcomp(scaled_features, center = TRUE, scale. = TRUE)

# plot the scree plot
fviz_eig(pca, addlabels = TRUE, barfill = "steelblue") +
  labs(title = "Scree Plot")

```
## Summary Statistics

```{r}
data6 <- read.csv("full_data_cleaned_without_nas.csv", header=TRUE) #loading data
data <- data6

# Summary stats variables
skim(data6)
```

## Histogram of Features

```{r}
# Histograms of some acoustic features
acoustic_subset <- data6 %>%
  select(loudness_sma3_amean,
         F0semitoneFrom27.5Hz_sma3nz_percentile50.0,
         shimmerLocaldB_sma3nz_amean,
         F1amplitudeLogRelF0_sma3nz_amean)

# histogram setup
acoustic_long <- acoustic_subset %>%
  pivot_longer(everything(), names_to = "Feature", values_to = "Value")

# create histograms
ggplot(acoustic_long, aes(x = Value)) +
  geom_histogram(bins = 30, fill = "steelblue", color = "white") +
  facet_wrap(~ Feature, scales = "free", ncol = 2) +
  theme_minimal() +
  labs(title = "Distribution of Selected Acoustic Features")

```

## Distribution After Cleaning Data: 
```{r}
# eda: dist after cleaing data
ggplot(data6, aes(x = Reason, fill = Reason)) +
  geom_bar() +
  theme_minimal() +
  labs(title = "Distribution of Parent-Labeled Cry Reasons", x = "Reason", y = "Count") +
  theme(axis.text.x = element_text(angle = 30, hjust = 1))

```

## Reason by Gender Plot

```{r}
# reason by gender plot
ggplot(data6, aes(x = Gender, fill = Reason)) +
  geom_bar(position = "fill") +
  scale_y_continuous(labels = scales::percent) +
  labs(title = "Cry Reasons by Gender (Proportion)", y = "Proportion") +
  theme_minimal()
```

## PCA Components

```{r}
pca_data <- data %>% # obtain pca data for narrowing variables
  select(starts_with("shimmer"),
         starts_with("F0"),
         starts_with("alpha"),
         starts_with("F1"),
         starts_with("F2")) %>%
  na.omit()

scaled_pca_data <- scale(pca_data) # scaling data

pca <- prcomp(scaled_pca_data, center = TRUE, scale. = TRUE)

# plot the pca
fviz_eig(pca, 
         addlabels = TRUE,
         barfill = "steelblue",
         barcolor = "black") +
  labs(title = "Scree Plot: Variance Explained by Principal Components",
       x = "Principal Component",
       y = "Percentage of Variance Explained")

```

```{r}
head(pca$rotation)
```

# Question 2: Unsupervised learning

### K-means Clustering Attempt 1

```{r}
data <- read.csv("./full_data_cleaned_without_nas.csv", header=TRUE) #loading data
data2 <- data
data2$Reason <- as.factor(data$Reason) # factor data

set.seed(333) # set up training data
trainIndex <- createDataPartition(data2$Reason, p = 0.8, list = FALSE)
trainData <- data2[trainIndex, ]
testData <- data2[-trainIndex, ]

ctrl <- trainControl(method = "cv", number = 10, verboseIter = FALSE)
```

```{r}
# tart fresh
data5 <- data

# only acoustic data
acoustic_numeric <- data5 %>%
  select(where(is.numeric))

# Scale the features
acoustic_scaled <- scale(acoustic_numeric)

#k-means clustering (k=5)
set.seed(123)
k <- 5
kmeans_res <- kmeans(acoustic_scaled, centers = k, nstart = 25)

# add cluster labels to data5
data5$Cluster <- factor(kmeans_res$cluster)

#print results
table(Cluster = data5$Cluster, Reason = data5$Reason)

```

```{r}
data5$Cluster <- factor(data5$Cluster)  # facotr data

#prepare dataset for classification
dataset_for_cluster_pred <- data5 %>%
  select(-Reason, -Gender, -Age, -newID, -Date, -Sample)  # keep acoustic + Cluster

# 10-fold cross-validation
set.seed(123)
ctrl <- trainControl(method = "cv", number = 10, verboseIter = FALSE)

# rando forest to predict clusters from features
cluster_pred_model <- train(
  Cluster ~ .,
  data = dataset_for_cluster_pred,
  method = "rf",
  trControl = ctrl
)

# CV accuracy for predicting clusters
mean(cluster_pred_model$resample$Accuracy)

```

### K means Clustering w/ PCA 

```{r}
# PCA
pca_kmeans <- prcomp(acoustic_scaled)

# fist two PCs
pca_df <- as.data.frame(pca_kmeans$x[, 1:2])
pca_df$Cluster <- factor(data5$Cluster)

# Plot
ggplot(pca_df, aes(x = PC1, y = PC2, color = Cluster)) +
  geom_point(alpha = 0.7, size = 2) +
  labs(title = "K-means Clustering of Baby Cries (PCA Projection)",
       x = "Principal Component 1",
       y = "Principal Component 2") +
  theme_minimal() +
  scale_color_brewer(palette = "Set1")

```

```{r}
# table for predicitng clusters
table(Cluster = data5$Cluster, Reason = data5$Reason)
```


```{r}
data7 <- data
# ACOUSTIC ONLY AGAIN
acoustic_only2 <- data7 %>%
  select(-Reason, -Gender, -Age, -newID, -ID, -Date, -Sample, -ParentFile)

acoustic_scaled <- scale(acoustic_only2)

# K-means clustering
set.seed(123)
kmeans_model <- kmeans(acoustic_scaled, centers = 5, nstart = 25)

# Add cluster labels
data7$Cluster <- as.factor(kmeans_model$cluster)

```

```{r}
set.seed(123)

ctrl <- trainControl(method = "cv", number = 10, verboseIter = FALSE)

# RF model using only Cluster to predict Reason
rf_cluster_model <- train(
  Reason ~ Cluster,
  data = data7,
  method = "rf",
  trControl = ctrl
)

# average cross-validated accuracy
mean(rf_cluster_model$resample$Accuracy)

# table for results
table_cluster_reason <- table(data7$Cluster, data7$Reason)
table_cluster_reason

# Convert to data frame for plotting

conf_df <- as.data.frame(table_cluster_reason)
colnames(conf_df) <- c("Cluster", "Reason", "Count")

ggplot(conf_df, aes(x = Reason, y = Cluster, fill = Count)) +
  geom_tile(color = "white") +
  geom_text(aes(label = Count), color = "black", size = 4) +
  scale_fill_gradient(low = "white", high = "steelblue") +
  labs(title = "Cluster vs Reason Heatmap",
       x = "Parent-Labeled Reason",
       y = "Cluster (from K-means)") +
  theme_minimal()

```

```{r}
# Create a contingency table
table(data7$Cluster, data7$Reason)

# data frame too
cluster_reason_df <- as.data.frame(table(data7$Cluster, data7$Reason))
print(cluster_reason_df)

# show proportions within each cluster
prop.table(table(data7$Cluster, data7$Reason), margin = 1)

```

### K-means Clustering Attempt 2

```{r}
# loading incomplete and complete data into R
data_with_nas <- read.csv("full_data_cleaned_include_nas.csv")
data_complete <- read.csv("full_data_cleaned_without_nas.csv")

# removing columns (newID, ID, Date, Sample, ParentFile)
rf_with_nas <- read.csv("full_data_cleaned_include_nas.csv")[, -c(1, 2, 6:8)]
rf_complete <- read.csv("full_data_cleaned_without_nas.csv")[, -c(1, 2, 4:8)]
rf_with_nas$Reason <- as.factor(rf_with_nas$Reason)
rf_complete$Reason <- as.factor(rf_complete$Reason)

# create method vector for mice
methods <- make.method(rf_with_nas)
rf_with_nas$Gender[rf_with_nas$Gender == "MULT-REDCAP-GENDER"] <- NA
rf_with_nas$Gender <- as.factor(rf_with_nas$Gender)
init <- mice(rf_with_nas, maxit = 0)  
methods <- init$method                
methods["Reason"] <- ""
methods["Gender"] <- "logreg"  # or "polyreg" if >2 levels

# impute missing values using mice
imputed_data <- mice(rf_with_nas, m = 1, method = methods, seed = 605794011)
data_imputed <- complete(imputed_data, action = 1)

# ensure target and gender are factors
data_imputed$Reason <- as.factor(data_imputed$Reason)
data_imputed$Gender <- as.factor(data_imputed$Gender)

# set seed for reproducibility
set.seed(605794011)

# train/test split
train_index2 <- createDataPartition(data_imputed$Reason, p = 0.8, list = FALSE)
train_imputed <- data_imputed[train_index2, ]
test_imputed <- data_imputed[-train_index2, ]
```

```{r k means}
# remove categorical columns
clust_data <- data_imputed %>%
  select_if(is.numeric)

# scale (standardize) the data
clust_data_scaled <- scale(clust_data)[, -1]

# set seed for reproducibility
set.seed(605794011)

# establish k as number of groups
k <- 5

# run k-means variable
kmeans_result <- kmeans(clust_data_scaled, centers = k, nstart = 25)
print(kmeans_result)

# assign modeled clusters to each data point
clustered_data <- data_imputed
clustered_data$Cluster <- as.factor(kmeans_result$cluster)

# visualize clusters
fviz_cluster(kmeans_result,
             data = clust_data_scaled,
             ellipse.type = "euclid",
             labelsize = 0,           
             show.clust.cent = TRUE,
             ggtheme = theme_minimal()) +
  labs(title = "K-means Clustering (k = 5)", x = "PC1", y = "PC2")

# run pca analysis
pca_result <- prcomp(clust_data, scale. = TRUE)

# calculate proportion of variance explained
pve <- (pca_result$sdev)^2 / sum(pca_result$sdev^2)

# plot proportion of variance for principal components
plot(pve, type = "b", pch = 19, col = "steelblue", lwd = 2,
     xlab = "Principal Component", 
     ylab = "Proportion of Variance Explained", 
     main = "PVE for Principal Components",
     cex.main = 1.2, cex.lab = 1.1, cex.axis = 0.9)

# add grid lines to graph
grid()

# label each point with exact variance value
text(x = 1:length(pve), y = pve, 
     labels = round(pve, 2), 
     pos = 3, cex = 0.4, col = "black")

# print proportion of variance for PC1 and PC2
pve[1:2]
pve[1]+pve[2]

# create a data frame with cluster assignments and true labels
df <- data.frame(Cluster = clustered_data$Cluster, CryType = data_imputed$Reason)

# generate a contingency table: distribution of true labels within each cluster
label_distribution <- table(df$Cluster, df$CryType)

# neat printed table
kable(label_distribution, caption = "Distribution of Cry Types Within Each Cluster")

# convert to proportions within each cluster
prop_distribution <- prop.table(label_distribution, margin = 1) 
# for proportions
kable(round(prop_distribution, 2), caption = "Proportion of Cry Types Within Each Cluster")

# data frame with cluster assignments, reasons and frequency associated with each cluster/reason combo
cluster_counts <- df %>%
  group_by(Cluster, CryType) %>%
  summarise(Frequency = n(), .groups = 'drop')

# plot each reason against the cluster assignments
ggplot(cluster_counts, aes(x = CryType, y = Frequency, fill = CryType)) +
  geom_bar(stat = "identity", show.legend = FALSE) +
  facet_wrap(~ Cluster, ncol = 2) +
  labs(
    title = "Distribution of Cry Reasons Within Each Cluster",
    x = "Cry Reason",
    y = "Frequency"
  ) +
  theme_minimal(base_size = 12) +
  theme(
    axis.text.x = element_text(angle = 30, hjust = 1),
    strip.text = element_text(face = "bold")
  )

# dimensions
dim(clust_data)
```

### Spectral Clustering 

```{r spectral clustering}

# generate random data for clustering
set.seed(605794011)
k <- 5

# extract the first two principal components
PC1 <- pca_result$x[, 1]
PC2 <- pca_result$x[, 2]

# compute the similarity matrix
similarity_matrix <- exp(-dist(clust_data)^2)

# perform spectral decomposition
eigen_result <- eigen(similarity_matrix)

# extract the top-k eigenvectors
k_eigenvectors <- eigen_result$vectors[, 1:k]

# perform k-means clustering on the eigenvectors
cluster_assignments <- kmeans(k_eigenvectors, centers = k)$cluster

# visualize clusters
cluster_colors <- rainbow(k)
plot(PC1, PC2, col = cluster_colors[cluster_assignments], pch = 19, cex = 0.7,
     main = "Spectral Clustering with k-means", xlab = "PC1", ylab = "PC2")
legend("topright", legend = paste("Cluster", 1:k), col = cluster_colors, pch = 19)


```

### Gaussian Mixture Models

```{r}
df <- read.csv("full_data_cleaned_without_nas.csv")

# Identify all numeric columns
numeric_cols <- sapply(df, is.numeric)
numeric_col_names <- names(df)[numeric_cols]

# Exclude the non-numeric/irrelevant columns
cols_to_exclude <- c("newID", "Age", "Date", "Sample")
numeric_data <- df[, setdiff(numeric_col_names, cols_to_exclude)]

# Perform GMM
gmm <- Mclust(numeric_data)

# Get the cluster assignments
cluster_assignments <- gmm$classification
```

```{r}
# Perform PCA on the data used for GMM
pca_res <- prcomp(numeric_data, scale. = TRUE) # scale.=TRUE is important for PCA

# Create a data frame for plotting PCA results with cluster assignments
pca_data <- as.data.frame(pca_res$x)
pca_data$cluster <- as.factor(cluster_assignments)
```

```{r}
# Plot the first two principal components, colored by cluster
table(cluster_assignments)
ggplot(pca_data, aes(x = PC1, y = PC2, color = cluster)) +
  geom_point(alpha = 0.7) +
  stat_ellipse(aes(group = cluster), type = "norm", linetype = 2) +
  labs(title = "GMM Clusters Visualized with PCA",
       x = paste0("Principal Component 1 (", round(summary(pca_res)$importance[2,1]*100, 2), "%)"),
       y = paste0("Principal Component 2 (", round(summary(pca_res)$importance[2,2]*100, 2), "%)")) +
  theme_minimal() +
  theme(legend.title = element_text(size = 10))
```


# Question 3: Supervised Learning

## Regression Models

```{r setup, include=FALSE}
data <- read.csv("full_data_cleaned_without_nas.csv", header=TRUE) #loading data
data2 <- data
```

```{r}
data2$Reason <- as.factor(data$Reason) # factor data

set.seed(333) # set up training data
trainIndex <- createDataPartition(data2$Reason, p = 0.8, list = FALSE)
trainData <- data2[trainIndex, ]
testData <- data2[-trainIndex, ]

ctrl <- trainControl(method = "cv", number = 10, verboseIter = FALSE)

set.seed(333) # logistic model, just a test!
model_logit <- train(
  Reason ~ . - ID,
  data = trainData,
  method = "multinom",
  trControl = ctrl
)

model_logit$results

```

```{r}
head(data)
```

### Logistic Regression

```{r}
# preferred model
data$Reason <- as.factor(data$Reason)
data$Gender <- as.factor(data$Gender)

data_clean <- data %>%
  select(-newID, -ID, -Date, -Sample) # remove categorical data

acoustic_features <- data_clean %>%
  select_if(is.numeric) %>%
  scale() %>%
  as.data.frame() # get the acoustic features

data_model <- bind_cols(acoustic_features, data %>% select(Reason, Gender, Age))

```

```{r}
set.seed(123)
logit_model <- multinom(Reason ~ ., data = data_model) # logstic model creation

preds <- predict(logit_model, newdata = data_model)

conf_mat <- confusionMatrix(preds, data_model$Reason) # produce confusion matrix

print(conf_mat$overall['Accuracy'])
print(conf_mat$table)

conf_df <- as.data.frame(conf_mat$table) # print the matrix
ggplot(conf_df, aes(Prediction, Reference, fill = Freq)) +
  geom_tile() +
  geom_text(aes(label = Freq), color = "white", size = 4) +
  scale_fill_gradient(low = "blue", high = "red") +
  theme_minimal() +
  labs(title = "Confusion Matrix")

```

### Linear Regression

```{r}
data_lm <- data %>% # linear model data
  select(loudness_sma3_amean, Age, Gender,
         starts_with("shimmer"),
         starts_with("F0"),
         starts_with("alpha"),
         starts_with("F1"),
         starts_with("F2"))

data_lm$Gender <- as.factor(data_lm$Gender)

lm_model <- lm(loudness_sma3_amean ~ ., data = data_lm) # set up model

summary(lm_model) # summarize

data_lm$Predicted <- predict(lm_model, newdata = data_lm)

# plot the model
ggplot(data_lm, aes(x = loudness_sma3_amean, y = Predicted)) +
  geom_point(alpha = 0.5) +
  geom_smooth(method = "lm", se = FALSE, color = "red") +
  theme_minimal() +
  labs(title = "Predicted vs Actual Cry Loudness",
       x = "Actual",
       y = "Predicted")


```

```{r}
# just a simple plot for more linear regression showcase
ggplot(data, aes(x = Reason, y = loudness_sma3_amean, fill = Reason)) +
  geom_boxplot(alpha = 0.7, outlier.shape = NA) +
  geom_jitter(width = 0.2, alpha = 0.3, color = "black") +
  theme_minimal() +
  labs(
    title = "Cry Loudness by Parent-Labeled Reason",
    x = "Cry Type (Reason)",
    y = "Average Loudness"
  ) +
  theme(axis.text.x = element_text(angle = 30, hjust = 1)) +
  guides(fill = FALSE)

```



### Non-Linear Supervised Learning Models

```{r loading data in}

# loading incomplete and complete data into R
data_with_nas <- read.csv("full_data_cleaned_include_nas.csv")
data_complete <- read.csv("full_data_cleaned_without_nas.csv")

# removing columns (newID, ID, Date, Sample, ParentFile)
rf_with_nas <- read.csv("full_data_cleaned_include_nas.csv")[, -c(1, 2, 6:8)]
rf_complete <- read.csv("full_data_cleaned_without_nas.csv")[, -c(1, 2, 4:8)]
rf_with_nas$Reason <- as.factor(rf_with_nas$Reason)
rf_complete$Reason <- as.factor(rf_complete$Reason)

```

### Random Forest

```{r random forest model on complete cases only using numeric variables}

# set seed for reproducibility
set.seed(605794011)

# train/test split
train_index <- createDataPartition(rf_complete$Reason, p = 0.8, list = FALSE)
train_complete <- rf_complete[train_index, ]
test_complete <- rf_complete[-train_index, ]

# fit random forest model
randomforest_complete <- randomForest(Reason ~ ., data = train_complete, importance = TRUE)

# predictions and evaluation of accuracy 
preds_complete <- predict(randomforest_complete, test_complete)
conf_matrix_complete <- confusionMatrix(preds_complete, test_complete$Reason)
accuracy_complete <- conf_matrix_complete$overall['Accuracy']

# show results
print(conf_matrix_complete)
print(accuracy_complete)
varImpPlot(randomforest_complete, main = "Variable Importance (Complete Data)", cex = 0.7)

# creating a confusion matrix heatmap (pretty)
cm_df <- as.data.frame(conf_matrix_complete$table)
ggplot(cm_df, aes(x = Reference, y = Prediction)) +
  geom_tile(aes(fill = Freq), color = "white") +
  geom_text(aes(label = Freq), size = 4) +
  scale_fill_gradient(low = "white", high = "steelblue") +
  theme_minimal() +
  labs(title = "Confusion Matrix Heatmap", x = "Actual", y = "Predicted")

```

### Random Forest w/ mice imputation

```{r random forest model on mice imputed data}

# create method vector for mice
methods <- make.method(rf_with_nas)
rf_with_nas$Gender[rf_with_nas$Gender == "MULT-REDCAP-GENDER"] <- NA
rf_with_nas$Gender <- as.factor(rf_with_nas$Gender)
init <- mice(rf_with_nas, maxit = 0)  
methods <- init$method                
methods["Reason"] <- ""
methods["Gender"] <- "logreg"  # or "polyreg" if >2 levels

# impute missing values using mice
imputed_data <- mice(rf_with_nas, m = 1, method = methods, seed = 605794011)
data_imputed <- complete(imputed_data, action = 1)

# ensure target and gender are factors
data_imputed$Reason <- as.factor(data_imputed$Reason)
data_imputed$Gender <- as.factor(data_imputed$Gender)

# set seed for reproducibility
set.seed(605794011)

# train/test split
train_index2 <- createDataPartition(data_imputed$Reason, p = 0.8, list = FALSE)
train_imputed <- data_imputed[train_index2, ]
test_imputed <- data_imputed[-train_index2, ]

# fit random forest model
rf_imputed <- randomForest(Reason ~ ., data = train_imputed, importance = TRUE)

# predictions and evaluations of accuracy
preds_imputed <- predict(rf_imputed, test_imputed)
conf_matrix_imputed <- confusionMatrix(preds_imputed, test_imputed$Reason)
accuracy_imputed <- conf_matrix_imputed$overall['Accuracy']

# show results
print(conf_matrix_imputed)
print(accuracy_imputed)
varImpPlot(rf_imputed, main = "Variable Importance (MICE Imputed Data)", cex = 0.7)

# creating a confusion matrix heatmap (pretty)
cm_df <- as.data.frame(conf_matrix_imputed$table)
ggplot(cm_df, aes(x = Reference, y = Prediction)) +
  geom_tile(aes(fill = Freq), color = "white") +
  geom_text(aes(label = Freq), size = 4) +
  scale_fill_gradient(low = "white", high = "steelblue") +
  theme_minimal() +
  labs(title = "Confusion Matrix Heatmap (Imputed Data)", x = "Actual", y = "Predicted")

# extract variable importance
var_imp <- importance(rf_imputed, type = 1)  # MeanDecreaseAccuracy
var_imp_df <- as.data.frame(var_imp)
colnames(var_imp_df) <- make.names(colnames(var_imp_df))  # Ensure clean column names
var_imp_df$Variable <- rownames(var_imp_df)

# select top 20 variables
top_n <- 20
var_imp_df <- as_tibble(var_imp_df) %>%
  arrange(desc(MeanDecreaseAccuracy)) %>%
  dplyr::slice(1:top_n)

# create pretty plot for variable importance
ggplot(var_imp_df, aes(x = reorder(Variable, MeanDecreaseAccuracy),
                       y = MeanDecreaseAccuracy)) +
  geom_col(fill = "#4682B4") +
  coord_flip() +
  theme_minimal(base_size = 13) +
  labs(title = "Top 20 Most Important Variables",
       subtitle = "Measured by Mean Decrease in Accuracy (Imputed Data)",
       x = NULL,
       y = "Mean Decrease in Accuracy") +
  theme(
    plot.title = element_text(hjust = 0.5, face = "bold", size = 15),
    plot.subtitle = element_text(hjust = 0.5, size = 12),
    axis.text.y = element_text(size = 8),
    axis.text.x = element_text(size = 11)
  )

```

### Random Forest w/ subset of features

```{r using only specific features}

# subset of variables from variable importance
rf_with_nas <- rf_with_nas[, c("Reason", "loudness_sma3_percentile50.0", "loudness_sma3_stddevNorm", "F2amplitudeLogRelF0_sma3nz_stddevNorm", "StddevUnvoicedSegmentLength", "loudness_sma3_percentile20.0", "F2amplitudeLogRelF0_sma3nz_amean", "MeanUnvoicedSegmentLength", "loudness_sma3_amean", "loudness_sma3_percentile80.0", "F1amplitudeLogRelF0_sma3nz_stddevNorm")]

# impute missing values using mice
imputed_data <- mice(rf_with_nas, m = 1, method = "pmm", seed = 605794011)
data_imputed <- complete(imputed_data, action = 1)

# ensure target and gender are factors
data_imputed$Reason <- as.factor(data_imputed$Reason)

# set seed for reproducibility
set.seed(605794011)

# train/test split
train_index2 <- createDataPartition(data_imputed$Reason, p = 0.8, list = FALSE)
train_imputed <- data_imputed[train_index2, ]
test_imputed <- data_imputed[-train_index2, ]

# fit random forest model with selected variables
rf_imputed <- randomForest(Reason ~ ., data = train_imputed, importance = TRUE)

# predictions and evaluations of accuracy
preds_imputed <- predict(rf_imputed, test_imputed)
conf_matrix_imputed <- confusionMatrix(preds_imputed, test_imputed$Reason)
accuracy_imputed <- conf_matrix_imputed$overall['Accuracy']

# show results
print(conf_matrix_imputed)
print(accuracy_imputed)
varImpPlot(rf_imputed, main = "Variable Importance (MICE Imputed Data)", cex = 0.7)

```

### XGBoost

```{r xgboost model}

# ensure Reason is a factor
data_imputed$Reason <- as.factor(data_imputed$Reason)

# clean up class levels to be valid R variable names
levels(data_imputed$Reason) <- make.names(levels(data_imputed$Reason))

# train/test split (optional if using cross-validation)
set.seed(605794011)
train_index <- createDataPartition(data_imputed$Reason, p = 0.8, list = FALSE)
train_data <- data_imputed[train_index, ]
test_data <- data_imputed[-train_index, ]

# cross-validation setup
ctrl <- trainControl(
  method = "cv",        # 10 fold cross-validaton
  number = 10,           
  classProbs = TRUE,    # needed for multiclass AUC etc.
  summaryFunction = multiClassSummary,
  verboseIter = FALSE
)

# train XGBoost model
set.seed(605794011)
xgb_model <- train(
  Reason ~ .,
  data = train_data,
  method = "xgbTree",
  trControl = ctrl,
  tuneLength = 5,       # tries 5 different combinations of params, trains 10 models per set of params, 10 x 5 = 50 models total
  metric = "Accuracy"
)

# print model summary and best tuning parameters
print(xgb_model)
plot(xgb_model)

# predict on test set
xgb_preds <- predict(xgb_model, newdata = test_data)

# confusion matrix and accuracy
conf_matrix <- confusionMatrix(xgb_preds, test_data$Reason)
print(conf_matrix)
accuracy <- conf_matrix$overall['Accuracy']
print(accuracy)
```

```{r ggplot}

# creating a confusion matrix heatmap (pretty)
cm_df <- as.data.frame(conf_matrix$table)
ggplot(cm_df, aes(x = Reference, y = Prediction)) +
  geom_tile(aes(fill = Freq), color = "white") +
  geom_text(aes(label = Freq), size = 4) +
  scale_fill_gradient(low = "white", high = "steelblue") +
  theme_minimal() +
  labs(title = "Confusion Matrix Heatmap (XGBoost)", x = "Actual", y = "Predicted")

# extract variable importance
xgb_imp <- varImp(xgb_model)$importance
xgb_imp$Variable <- rownames(xgb_imp)

# choose top 20 important features
top_n <- 20
xgb_imp <- as_tibble(xgb_imp) %>%
  arrange(desc(Overall)) %>%
  dplyr::slice(1:top_n)

# make a clean ggplot for variable importance
ggplot(xgb_imp, aes(x = reorder(Variable, Overall), y = Overall)) +
  geom_col(fill = "#1f78b4") +
  coord_flip() +
  theme_minimal(base_size = 13) +
  labs(title = "Top 20 Variable Importances",
       subtitle = "XGBoost Model (Caret)",
       x = NULL,
       y = "Importance Score") +
  theme(
    plot.title = element_text(hjust = 0.5, face = "bold", size = 15),
    plot.subtitle = element_text(hjust = 0.5, size = 12),
    axis.text.y = element_text(size = 9),
    axis.text.x = element_text(size = 11)
  )

```
